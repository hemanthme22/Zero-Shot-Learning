{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from VGG19 for all Train, Validation, and Test. Saving Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list of all image paths for train and validation data.\n",
    "from os import walk\n",
    "from os.path import normpath, basename\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "train_path_list = []\n",
    "train_dir_list = []\n",
    "val_path_list = []\n",
    "val_dir_list = []\n",
    "test_path_list = []\n",
    "test_dir_list = []\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "train_data_dir = os.path.join(data_dir, 'train_data')\n",
    "validation_data_dir = os.path.join(data_dir, 'validation_data')\n",
    "test_data_dir = os.path.join(data_dir, 'test_data')\n",
    "\n",
    "#Generating Train data path list\n",
    "for (dirpath, dirnames, filenames) in walk(train_data_dir):\n",
    "    for filename in filenames:\n",
    "        train_dir_list.append(basename(normpath(dirpath)))\n",
    "        train_path_list.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "#Generating Validation data path list\n",
    "for (dirpath, dirnames, filenames) in walk(validation_data_dir):\n",
    "    for filename in filenames:\n",
    "        val_dir_list.append(basename(normpath(dirpath)))\n",
    "        val_path_list.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "#Generating Test data path list\n",
    "for (dirpath, dirnames, filenames) in walk(test_data_dir):\n",
    "    for filename in filenames:\n",
    "        test_dir_list.append(basename(normpath(dirpath)))\n",
    "        test_path_list.append(os.path.join(dirpath, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting lists into dataframes\n",
    "import pandas as pd\n",
    "\n",
    "train_data_df = pd.DataFrame()\n",
    "train_data_df['image_paths'] = train_path_list\n",
    "train_data_df['class_name'] = train_dir_list\n",
    "\n",
    "val_data_df = pd.DataFrame()\n",
    "val_data_df['image_paths'] = val_path_list\n",
    "val_data_df['class_name'] = val_dir_list\n",
    "\n",
    "\n",
    "print(train_data_df.shape)\n",
    "print(val_data_df.shape)\n",
    "\n",
    "test_data_df = pd.DataFrame()\n",
    "test_data_df['image_paths'] = test_path_list\n",
    "test_data_df['class_name'] = test_dir_list\n",
    "\n",
    "print(test_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for extracting features using VGG19 for a given image path\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "base_model = VGG19(weights='imagenet')\n",
    "#print(base_model.summary())\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)\n",
    "\n",
    "def feature_extract_vgg19(image_path,model_keras):\n",
    "\n",
    "    img_path = image_path\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    flatten_features = model_keras.predict(x)\n",
    "    return(flatten_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(val_data_df.shape)\n",
    "#print(val_data_df.columns)\n",
    "#sub_df = val_data_df[val_data_df['class_name']=='valley']\n",
    "#print(sub_df.shape)\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.width', None)\n",
    "#pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#new_sub_df = sub_df[~sub_df.image_paths.str.contains('/.jpg',regex = False)]\n",
    "\n",
    "#print(new_sub_df.shape)\n",
    "#print(new_sub_df['image_paths'])\n",
    "\n",
    "train_data_df = train_data_df[~train_data_df.image_paths.str.contains('/.jpg',regex = False)]\n",
    "\n",
    "val_data_df = val_data_df[~val_data_df.image_paths.str.contains('/.jpg',regex = False)]\n",
    "\n",
    "test_data_df = test_data_df[~test_data_df.image_paths.str.contains('/.jpg',regex = False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_df.shape)\n",
    "print(val_data_df.shape)\n",
    "print(test_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting features and creating dataframe \n",
    "\n",
    "train_data_df['img_features'] = [feature_extract_vgg19(x,model_keras = model) for x in train_data_df['image_paths']]\n",
    "\n",
    "val_data_df['img_features'] = [feature_extract_vgg19(x,model_keras = model) for x in val_data_df['image_paths']]\n",
    "\n",
    "test_data_df['img_features'] = [feature_extract_vgg19(x,model_keras = model) for x in test_data_df['image_paths']]\n",
    "\n",
    "print(train_data_df.shape)\n",
    "print(val_data_df.shape)\n",
    "print(test_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Train and Validation features dataframes\n",
    "import pickle\n",
    "\n",
    "train_data_df.to_pickle(data_dir+'/train_data_features_df.pkl')\n",
    "val_data_df.to_pickle(data_dir+'/val_data_features_df.pkl')\n",
    "test_data_df.to_pickle(data_dir+'/test_data_features_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training base line model and Saving (entire data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Saved Train and Validation feature dataframes\n",
    "import pickle\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "\n",
    "train_data_df = pd.read_pickle(data_dir+'/train_data_features_df.pkl')\n",
    "val_data_df = pd.read_pickle(data_dir+'/val_data_features_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.reset_index(drop=True)\n",
    "print(len(train_data_df))\n",
    "print(train_data_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = train_data_df.iloc[0:100000]\n",
    "df2 = train_data_df.iloc[100000:200000]\n",
    "df3 = train_data_df.iloc[200000:300000]\n",
    "df4 = train_data_df.iloc[300000:400000]\n",
    "df5 = train_data_df.iloc[400000:500000]\n",
    "df6 = train_data_df.iloc[500000:615827]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_train = df1.img_features.apply(pd.Series)\n",
    "df2_train = df2.img_features.apply(pd.Series)\n",
    "df3_train = df3.img_features.apply(pd.Series)\n",
    "df4_train = df4.img_features.apply(pd.Series)\n",
    "df5_train = df5.img_features.apply(pd.Series)\n",
    "df6_train = df6.img_features.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([df1_train,df2_train,df3_train,df4_train,df5_train,df6_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "del df1_train,df2_train,df3_train,df4_train,df5_train,df6_train,df1,df2,df3,df4,df5,df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multiprocessing as mp\n",
    "\n",
    "#print(train_data_df.shape)\n",
    "\n",
    "#p = mp.Pool(mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting array column of features into multiple columns\n",
    "\n",
    "#X_train = train_data_df.img_features.apply(pd.Series)\n",
    "#X_train = p.map(pd.Series, train_data_df['img_features'])\n",
    "y_train = train_data_df['class_name'].astype('category')\n",
    "\n",
    "X_val = val_data_df.img_features.apply(pd.Series)\n",
    "#X_val = p.map(pd.Series, val_data_df['img_features'])\n",
    "y_val = val_data_df['class_name'].astype('category')\n",
    "\n",
    "X_train_val = pd.concat([X_train,X_val])\n",
    "y_train_val = pd.concat([y_train,y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Train and Validation features dataframes\n",
    "import pickle\n",
    "\n",
    "X_train_val.to_pickle(data_dir+'/train_val_x_df.pkl')\n",
    "y_train_val.to_pickle(data_dir+'/train_val_y_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "\n",
    "X_train_val = pd.read_pickle(data_dir+'/train_val_x_df.pkl')\n",
    "y_train_val = pd.read_pickle(data_dir+'/train_val_y_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 19.3min\n"
     ]
    }
   ],
   "source": [
    "#Training RandomForest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#training randomforest\n",
    "mdl_rf = RandomForestClassifier(n_estimators=1000,random_state=0,verbose=1,n_jobs=-1, min_samples_split= 2, min_samples_leaf= 1, max_features= 'auto', max_depth= 60, bootstrap= False)\n",
    "    \n",
    "clf_fit = mdl_rf.fit(X_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Saving baseline model\n",
    "pickle.dump(clf_fit, open(data_dir+'/rf_baseline_trained.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using saved baseline model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Saved Test feature dataframe\n",
    "import pickle\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "\n",
    "test_data_df = pd.read_pickle(data_dir+'/test_data_features_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Saved baseline Model\n",
    "\n",
    "clf_fit = pickle.load(open(data_dir+'/rf_baseline_trained.sav', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data_df.img_features.apply(pd.Series)\n",
    "y_test = test_data_df['class_name'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on test data\n",
    "yhat_clf = clf_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a confusion matrix on predictions\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "print(classification_report(y_test, yhat_clf))\n",
    "print(accuracy_score(y_test, yhat_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_clf_prob = clf_fit.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_predictions = np.argsort(yhat_clf_prob, axis = 1)[:,-3:]\n",
    "    \n",
    "#then find the associated SOC code for each prediction\n",
    "top_class = clf_fit.classes_[top_n_predictions]\n",
    "top_class_df = pd.DataFrame(data=top_class,columns=['top1','top2','top3'])\n",
    "print(top_class_df.shape)\n",
    "print(top_class_df.head(10))\n",
    "\n",
    "#merge it up with the validation labels and descriptions\n",
    "results = pd.merge(pd.DataFrame(y_test), top_class_df, left_index=True, right_index=True)\n",
    "print(results.shape)\n",
    "print(results.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
