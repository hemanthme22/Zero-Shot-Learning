{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classifiers for each values of K using saved features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "\n",
    "train_data_df = pd.read_pickle(data_dir+'/train_data_features_df.pkl')\n",
    "val_data_df = pd.read_pickle(data_dir+'/val_data_features_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining train and val data\n",
    "train_val_data_df = pd.concat([train_data_df,val_data_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   59.6s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  5.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 10.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f63adb3dd9ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#Saving baseline model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trained_models/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mmodelName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.sav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training a classifier for each value of K.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "f = open(\"fasttext/clusterCenters.txt\",'r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "  \n",
    "    line = line.split()\n",
    "    modelName = line[0]\n",
    "    classesNow = line[1:]\n",
    "    print(modelName)\n",
    "    \n",
    "    #Subsetting dataframe for only the classes being used now.\n",
    "    train_now_df = train_val_data_df[train_val_data_df['class_name'].isin(classesNow)]\n",
    "    \n",
    "    X_train_val = train_now_df.img_features.apply(pd.Series)\n",
    "    y_train_val = train_now_df['class_name'].astype('category')\n",
    "\n",
    "    #training randomforest\n",
    "    mdl_rf = RandomForestClassifier(n_estimators=1000,random_state=0,verbose=1,n_jobs=-1, min_samples_split= 2, min_samples_leaf= 1, max_features= 'auto', max_depth= 60, bootstrap= False)\n",
    "    \n",
    "    clf_fit = mdl_rf.fit(X_train_val, y_train_val)\n",
    "\n",
    "    #Saving baseline model\n",
    "    pickle.dump(clf_fit, open('trained_models/'+ modelName + '.sav', 'wb'))\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Trained classifiers to predict on test data for each K. Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "\n",
    "test_data_df = pd.read_pickle(data_dir+'/test_data_features_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data_df.img_features.apply(pd.Series)\n",
    "y_test = test_data_df['class_name'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"fasttext/clusterCenters.txt\",'r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.split()\n",
    "    modelName = line[0]\n",
    "    classesNow = line[1:]\n",
    "    print(modelName)\n",
    "    \n",
    "    clf_fit = pickle.load(open('trained_models/'+ modelName + '.sav', 'rb')) \n",
    "    \n",
    "    # evaluate the model on test data\n",
    "    yhat_clf = clf_fit.predict(X_test)\n",
    "    \n",
    "    pred_df = pd.DataFrame(data=yhat_clf, index=test_data_df['image_paths'], columns=['max_prob'])\n",
    "    pred_df.to_pickle('predictions/'+modelName+'.pkl') \n",
    "    \n",
    "    #Finding prob predictions for all classes\n",
    "    yhat_clf_prob = clf_fit.predict_proba(X_test)\n",
    "    \n",
    "    pred_df = pd.DataFrame(data=yhat_clf_prob, index=test_data_df['image_paths'], columns=clf_fit.classes_)\n",
    "    pred_df.to_pickle('predictions/all_categories/'+modelName+'.pkl') \n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating close word dict from FastText for each K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding closest words to top predictions on testing set\n",
    "import math\n",
    "import pickle\n",
    "from scipy.spatial import distance\n",
    "#from itertools import islice\n",
    "\n",
    "#def take(n, iterable):\n",
    "#    \"Return first n items of the iterable as a list\"\n",
    "#    return list(islice(iterable, n))\n",
    "\n",
    "def scipy_distance(v, u):\n",
    "    return distance.euclidean(v, u)\n",
    "\n",
    "#Reading the fasttext dictionary populated at clustering phase\n",
    "fastext_dict = pickle.load(open(\"fasttext/fastext_dict.pkl\",\"rb\"))\n",
    "print(len(fastext_dict))\n",
    "#print(fastext_dict.keys())\n",
    "#print(fastext_dict['car'])\n",
    "\n",
    "#total_classes = 379\n",
    "\n",
    "dict_keys = list(fastext_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the close words dictionary for all dictionary keys\n",
    "\n",
    "closeWords_Count = 6\n",
    "    \n",
    "closeWord_dict = {}\n",
    "    \n",
    "for word in dict_keys:\n",
    "    distance_dict = {}\n",
    "        \n",
    "    for fast_word in dict_keys:\n",
    "        dist = scipy_distance(fastext_dict[word],fastext_dict[fast_word])\n",
    "        distance_dict[fast_word] = dist\n",
    "            \n",
    "        #sorted_distace_dict = {k: v for k, v in sorted(distance_dict.items(), key=lambda item: item[1],reverse = True)[:closeWords_Count+1]}\n",
    "    closeWords_dict = {k: v for k, v in sorted(distance_dict.items(), key=lambda item: item[1])[:closeWords_Count]}\n",
    "        \n",
    "    closeWord_dict[word] = list(closeWords_dict.keys())\n",
    "    \n",
    "pickle.dump(closeWord_dict, open('close_word_dict/closeWord_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the close words dictionary for each model\n",
    "\n",
    "closeWords_Count = 6\n",
    "\n",
    "f = open(\"fasttext/clusterCenters.txt\",'r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    line = line.split()\n",
    "    modelName = line[0]\n",
    "    print(modelName)\n",
    "    classesNow = line[1:]\n",
    "    \n",
    "    closeWord_dict = {}\n",
    "    \n",
    "    for word in classesNow:\n",
    "        distance_dict = {}\n",
    "        \n",
    "        for fast_word in dict_keys:\n",
    "            dist = scipy_distance(fastext_dict[word],fastext_dict[fast_word])\n",
    "            distance_dict[fast_word] = dist\n",
    "            \n",
    "        #sorted_distace_dict = {k: v for k, v in sorted(distance_dict.items(), key=lambda item: item[1],reverse = True)[:closeWords_Count+1]}\n",
    "        closeWords_dict = {k: v for k, v in sorted(distance_dict.items(), key=lambda item: item[1])[:closeWords_Count]}\n",
    "        \n",
    "        closeWord_dict[word] = list(closeWords_dict.keys())\n",
    "    \n",
    "    pickle.dump(closeWord_dict, open('close_word_dict/'+ modelName + '_closeWord_dict.pkl', 'wb'))\n",
    "           \n",
    "    #pred_df = pd.read_csv('predictions/'+modelName+'.txt', header=True, index=True, sep=',')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running final predictions from classifier and close word dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "\n",
    "test_data_df = pd.read_pickle(data_dir+'/test_data_features_df.pkl')\n",
    "y_test_df = pd.DataFrame(test_data_df.set_index('image_paths').class_name)\n",
    "\n",
    "closeWord_dict = pickle.load(open('close_word_dict/closeWord_dict.pkl',\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running final predictions for top 3 predictions from classifier\n",
    "h = open(\"Kmodels_final_accuracy.txt\", \"w\")\n",
    "\n",
    "f = open(\"fasttext/clusterCenters.txt\",'r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    line = line.split()\n",
    "    modelName = line[0]\n",
    "    print(modelName)\n",
    "    \n",
    "    #Reading the predictions for each model\n",
    "    pred_df = pd.read_pickle('predictions/all_categories/'+modelName+'.pkl')\n",
    "    \n",
    "    #Finding top 3 predictions\n",
    "    top_n_predictions = np.argsort(pred_df.values, axis = 1)[:,-3:]\n",
    "    #then find the associated code for each prediction\n",
    "    top_class = pred_df.columns[top_n_predictions]\n",
    "    top_class_df = pd.DataFrame(data=top_class,columns=['top1','top2','top3'],index = pred_df.index)\n",
    "\n",
    "    results = pd.merge(y_test_df, top_class_df, left_index=True, right_index=True)\n",
    "    \n",
    "    #closeWord_dict = pickle.load(open('close_word_dict/'+ modelName + '_closeWord_dict.pkl',\"rb\"))\n",
    "    \n",
    "    results['guesses_1'] = results['top1'].map(closeWord_dict)\n",
    "    results['guesses_2'] = results['top2'].map(closeWord_dict)\n",
    "    results['guesses_3'] = results['top3'].map(closeWord_dict)\n",
    "    \n",
    "    pred_check = []\n",
    "    \n",
    "    #pred_df['pred_check'] = np.where(pred_df['actual_label'] in pred_df['guesses'],1,0)\n",
    "    for index,row in results.iterrows():\n",
    "        if (row['class_name'] in row['guesses_1']) or (row['class_name'] in row['guesses_2']) or (row['class_name'] in row['guesses_3']):\n",
    "            pred_check.append(1)\n",
    "        else:\n",
    "            pred_check.append(0)\n",
    "        \n",
    "    results['pred_check'] = pred_check\n",
    "    \n",
    "    total_right = results['pred_check'].sum()\n",
    "    total_rows = len(pred_df)\n",
    "    accuracy = round(total_right/total_rows,4)\n",
    "    \n",
    "    h.write(str(modelName) + ',' + str(accuracy) + '\\n')\n",
    "    \n",
    "f.close()\n",
    "h.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running final predictions for single predictions\n",
    "h = open(\"Kmodels_singlePred_final_accuracy.txt\", \"w\")\n",
    "\n",
    "f = open(\"fasttext/clusterCenters.txt\",'r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    line = line.split()\n",
    "    modelName = line[0]\n",
    "    print(modelName)\n",
    "    \n",
    "    #Reading the predictions for each model\n",
    "    pred_df = pd.read_pickle('predictions/'+modelName+'.pkl')\n",
    "\n",
    "    results = pd.merge(y_test_df, pred_df, left_index=True, right_index=True)\n",
    "    \n",
    "    closeWord_dict = pickle.load(open('close_word_dict/'+ modelName + '_closeWord_dict.pkl',\"rb\"))\n",
    "    \n",
    "    results['guesses'] = results['max_prob'].map(closeWord_dict)\n",
    "    \n",
    "    pred_check = []\n",
    "    \n",
    "    #pred_df['pred_check'] = np.where(pred_df['actual_label'] in pred_df['guesses'],1,0)\n",
    "    for index,row in results.iterrows():\n",
    "        if row['class_name'] in row['guesses']:\n",
    "            pred_check.append(1)\n",
    "        else:\n",
    "            pred_check.append(0)\n",
    "        \n",
    "    results['pred_check'] = pred_check\n",
    "    \n",
    "    total_right = results['pred_check'].sum()\n",
    "    total_rows = len(pred_df)\n",
    "    accuracy = round(total_right/total_rows,4)\n",
    "    \n",
    "    h.write(str(modelName) + ',' + str(accuracy) + '\\n')\n",
    "    \n",
    "f.close()\n",
    "h.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
