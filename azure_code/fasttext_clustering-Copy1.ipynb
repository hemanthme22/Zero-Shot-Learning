{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting list of all Categories to consider from nounsFinal.txt\n",
    "labels = open ('nounsFinal-Updated.txt', 'r+')\n",
    "lables_list = []\n",
    "for line in labels.readlines():\n",
    "    lables_list.append(line.split(' ')[0].rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lables_list))\n",
    "print(lables_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if all categories are present in training, validation and testing\n",
    "import os\n",
    "\n",
    "data_dir = os.path.join('/data/home/karan1234/notebooks/Karan-Paper','BlobStorage')\n",
    "validation_data_dir = os.path.join(data_dir, 'validation_data')\n",
    "train_data_dir = os.path.join(data_dir, 'train_data')\n",
    "test_data_dir = os.path.join(data_dir,'test_data')\n",
    "#print(train_data_dir,validation_data_dir,test_data_dir)\n",
    "\n",
    "train_classes = os.listdir(train_data_dir)\n",
    "test_classes = os.listdir(test_data_dir)\n",
    "valid_classes = os.listdir(validation_data_dir)\n",
    "\n",
    "print(len(train_classes))\n",
    "#print(train_classes[1:])\n",
    "\n",
    "print(len(valid_classes))\n",
    "#print(valid_classes[1:])\n",
    "\n",
    "print(len(test_classes))\n",
    "#print(test_classes[1:])\n",
    "\n",
    "#Finding classes in NounsFinal but not in Train\n",
    "def Diff(li1, li2): \n",
    "    return (list(set(li1) - set(li2))) \n",
    "\n",
    "print('NounsFinal vs Train')\n",
    "print(Diff(lables_list, train_classes))\n",
    "print(Diff(train_classes, lables_list))\n",
    "\n",
    "print('NounsFinal vs Validation')\n",
    "print(Diff(lables_list, valid_classes))\n",
    "\n",
    "print('Train vs Validation')\n",
    "print(Diff(train_classes, valid_classes))\n",
    "\n",
    "print('Train vs Test')\n",
    "print(Diff(train_classes, test_classes))\n",
    "\n",
    "print('NounsFinal vs Test')\n",
    "print(Diff(lables_list, test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding categories that may be misspelled\n",
    "!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(lables_list)\n",
    "print(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if all categories are present in training, validation and testing\n",
    "import os\n",
    "\n",
    "data_dir = os.path.join('/data/home/karan1234/notebooks/Karan-Paper','BlobStorage')\n",
    "validation_data_dir = os.path.join(data_dir, 'validation_data')\n",
    "train_data_dir = os.path.join(data_dir, 'train_data')\n",
    "test_data_dir = os.path.join(data_dir,'test_data')\n",
    "\n",
    "print(train_data_dir)\n",
    "\n",
    "import os\n",
    "total_size = 0\n",
    "for root, dirs, files in os.walk(train_data_dir):\n",
    "    for f in files:\n",
    "        total_size += os.path.getsize(os.path.join(root, f))\n",
    "print(total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing 1\n",
    "#Looking for a particular word from fasttext file\n",
    "fastext = open('wiki.en.vec', 'r+')\n",
    "#fastext_dict = {}\n",
    "counter = 0\n",
    "for line in fastext:\n",
    "#    if counter < 5:\n",
    "    \n",
    "    splitLine = line.split()\n",
    "    word = splitLine[0]\n",
    "    if word in ['book']:\n",
    "        embedding = [float(el) for el in splitLine[1:]]\n",
    "        print(word)\n",
    "        print(embedding)\n",
    "   # else:\n",
    "    #    break\n",
    "    #counter +=1\n",
    "fastext.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing 2\n",
    "#Looping through category list and finding from fastext file\n",
    "#fastext = open('wiki.en.vec', 'r+')\n",
    "fastext_dict = {}\n",
    "counter = 0\n",
    "for lookup_word in lables_list:\n",
    "    print(lookup_word)\n",
    "    fastext = open('wiki.en.vec', 'r+')\n",
    "    for line in fastext:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            if word == lookup_word:\n",
    "                print(word+' '+str(counter))\n",
    "                embedding = [float(el) for el in splitLine[1:]]\n",
    "                fastext_dict[word] = embedding\n",
    "                counter += 1\n",
    "                break\n",
    "    fastext.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping through fastext file and finding matches with category list\n",
    "#This method is faster but gives error at the end for some reason.Check!\n",
    "fastext = open('wiki.en.vec', 'r+')\n",
    "fastext_dict = {}\n",
    "counter = 0\n",
    "for line in fastext:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        if word in lables_list:\n",
    "            print(word+' '+str(counter))\n",
    "            embedding = [float(el) for el in splitLine[1:]]\n",
    "            fastext_dict[word] = embedding\n",
    "            counter += 1\n",
    "        else:\n",
    "            continue\n",
    "fastext.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method from orginal code of Karan\n",
    "def file_reader(file_name):\n",
    "    fastext = open(file_name, 'r+')\n",
    "    line = fastext.readline()\n",
    "    counter = 0\n",
    "    #fastext_dict = {}\n",
    "\n",
    "    while line :\n",
    "        line = fastext.readline().split(\" \")\n",
    "       # print(line)\n",
    "        if line[0] in lables_list:\n",
    "            yield line       \n",
    "            print(line[0]+' '+counter)\n",
    "            if counter == 399:\n",
    "               break\n",
    "            counter += 1\n",
    "    fastext.close()\n",
    "\n",
    "#fastext_dict[line[0]] = [float(el) for el in line[1:-1]]\n",
    "#fastext_dict = {}\n",
    "#for line in file_reader('wiki.en.vec'):\n",
    "    # print(line)\n",
    "#    fastext_dict[line[0]] = [float(el) for el in line[1:-1]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fastext_dict))\n",
    "#print(fastext_dict.keys())\n",
    "print(fastext_dict['car'])\n",
    "\n",
    "#Saving fastext_dict for future use in testing code\n",
    "import pickle\n",
    "f = open(\"fastext_dict.pkl\",\"wb\")\n",
    "pickle.dump(fastext_dict,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading fastest_dict\n",
    "import pickle\n",
    "\n",
    "fastext_dict = pickle.load(open('/data/home/karan1234/notebooks/Karan-Paper/fasttext/fastext_dict.pkl',\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379\n"
     ]
    }
   ],
   "source": [
    "print(len(fastext_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import math\n",
    "X = []\n",
    "keys = []\n",
    "for k, v in fastext_dict.items():\n",
    "    #print(type(v[0]))\n",
    "    X.append(v)\n",
    "    keys.append(k)\n",
    "  #  print(k,v)\n",
    "    \n",
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means\n",
    "kClusterValues= [i for i in np.arange(10,250,5)]\n",
    "#kClusterValues = [5,20,35]\n",
    "\n",
    "f = open(\"clusterCenters.txt\",'w')\n",
    "\n",
    "for k in kClusterValues:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "    #print(kmeans.cluster_centers_)\n",
    "    #print(kmeans.labels_)\n",
    "    centroids  = kmeans.cluster_centers_  #means of shape [10,] \n",
    "    #for centroid in centroids:\n",
    "      #  print(centroid)\n",
    "    uniq_label = list(set(kmeans.labels_))\n",
    "    keyList = []\n",
    "    for index_label, label in enumerate(uniq_label):\n",
    "        maxSim = 0\n",
    "        maxIndex = -99\n",
    "        cluster = []\n",
    "        for index,label1 in enumerate(kmeans.labels_):\n",
    "\n",
    "            if label == label1:\n",
    "                cluster.append(keys[index])\n",
    "                if maxSim < cosine_similarity(X[index],centroids[index_label]):\n",
    "                   maxSim = cosine_similarity(X[index],centroids[index_label])\n",
    "                   maxIndex = index\n",
    "\n",
    "                   #print(maxIndex)\n",
    "                   #print(cosine_similarity(X[index],centroids[index_label]))\n",
    "        # print(keys[maxIndex])\n",
    "        keyList.append(keys[maxIndex])\n",
    "       # print(cluster)\n",
    "    strK = \"model\" + str(k) + \"  \" + \" \".join(keyList) +\"\\n\"\n",
    "    f.write(strK)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means with selecting K using elbow method\n",
    "cost =[] \n",
    "for i in range(5, 370): \n",
    "    KM = KMeans(n_clusters = i, max_iter = 500) \n",
    "    KM.fit(X) \n",
    "      \n",
    "    # calculates squared error \n",
    "    # for the clustered points \n",
    "    cost.append(KM.inertia_)      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  \n",
    "from matplotlib import style \n",
    "# plot the cost against K values \n",
    "plt.plot(range(5, 370), cost, color ='g', linewidth ='3') \n",
    "plt.xlabel(\"Value of K\") \n",
    "plt.ylabel(\"Sqaured Error (Cost)\") \n",
    "plt.show() # clear the plot \n",
    "# the point of the elbow is the  \n",
    "# most optimal value for choosing k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dendrogram for Heirarchial clustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method  = \"ward\"))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing k = 7\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "hclust = AgglomerativeClustering(n_clusters=7, affinity='cosine', linkage='average').fit(X)\n",
    "uniq_label = list(set(hclust.labels_))\n",
    "centroids  = hclust.cluster_centers_  \n",
    "print(uniq_label)\n",
    "   \n",
    "keyList = []\n",
    "for index_label, label in enumerate(uniq_label):\n",
    "    maxSim = 0\n",
    "    maxIndex = -99\n",
    "    cluster = []\n",
    "    for index,label1 in enumerate(hclust.labels_):\n",
    "\n",
    "        if label == label1:\n",
    "            cluster.append(keys[index])\n",
    "            if maxSim < cosine_similarity(X[index],centroids[index_label]):\n",
    "                maxSim = cosine_similarity(X[index],centroids[index_label])\n",
    "                maxIndex = index\n",
    "\n",
    "                   #print(maxIndex)\n",
    "                   #print(cosine_similarity(X[index],centroids[index_label]))\n",
    "        # print(keys[maxIndex])\n",
    "    keyList.append(keys[maxIndex])\n",
    "       # print(cluster)\n",
    "strK = \"model\" + str(k) + \"  \" + \" \".join(keyList) +\"\\n\"\n",
    "f.write(strK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the point of the elbow is the  \n",
    "# most optimal value for choosing k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GMM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
