{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train, Validation Features and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading ClusterCenters and finding the classes to train on.\n",
    "import os \n",
    "import numpy as np \n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "validation_data_dir = os.path.join(data_dir, 'validation_data')\n",
    "train_data_dir = os.path.join(data_dir, 'train_data')\n",
    "test_data_dir = os.path.join(data_dir, 'test_data')\n",
    "f = open(\"fasttext/clusterCenters.txt\",'r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "#print(lines)\n",
    "\n",
    "line = lines[0].split()\n",
    "print(line)\n",
    "modelName = line[0]\n",
    "kValue = int(modelName[5:])\n",
    "classesNow = line[1:]\n",
    "print(modelName)\n",
    "print(kValue)\n",
    "print(classesNow)\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list of all image paths for train and validation data.\n",
    "from os import walk\n",
    "from os.path import normpath, basename\n",
    "\n",
    "train_path_list = []\n",
    "train_dir_list = []\n",
    "val_path_list = []\n",
    "val_dir_list = []\n",
    "\n",
    "#Generating Train data path list\n",
    "for (dirpath, dirnames, filenames) in walk(train_data_dir):\n",
    "    for filename in filenames:\n",
    "        train_dir_list.append(basename(normpath(dirpath)))\n",
    "        train_path_list.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "#print(len(train_path_list))\n",
    "#print(train_path_list[2])\n",
    "#print(len(train_dir_list))\n",
    "#print(train_dir_list[2])\n",
    "\n",
    "#Generating Validation data path list\n",
    "for (dirpath, dirnames, filenames) in walk(validation_data_dir):\n",
    "    for filename in filenames:\n",
    "        val_dir_list.append(basename(normpath(dirpath)))\n",
    "        val_path_list.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "#print(len(val_path_list))\n",
    "#print(val_path_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting lists into dataframes\n",
    "import pandas as pd\n",
    "\n",
    "train_data_df = pd.DataFrame()\n",
    "train_data_df['image_paths'] = train_path_list\n",
    "train_data_df['class_name'] = train_dir_list\n",
    "\n",
    "val_data_df = pd.DataFrame()\n",
    "val_data_df['image_paths'] = val_path_list\n",
    "val_data_df['class_name'] = val_dir_list\n",
    "\n",
    "#train_data_df.describe()\n",
    "#val_data_df.describe()\n",
    "#print(train_data_df.loc[[243000]])\n",
    "#print(val_data_df.loc[[15100]])\n",
    "\n",
    "print(train_data_df.shape)\n",
    "print(val_data_df.shape)\n",
    "\n",
    "#Subsetting dataframes for only the classes being used now.\n",
    "train_data_df = train_data_df[train_data_df['class_name'].isin(classesNow)]\n",
    "val_data_df = val_data_df[val_data_df['class_name'].isin(classesNow)]\n",
    "\n",
    "print(train_data_df.shape)\n",
    "print(val_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if GPU is available\n",
    "#from keras import backend as K\n",
    "#K.tensorflow_backend._get_available_gpus()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for extracting features using VGG19 for a given image path\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "base_model = VGG19(weights='imagenet')\n",
    "#print(base_model.summary())\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)\n",
    "\n",
    "def feature_extract_vgg19(image_path,model_keras):\n",
    "\n",
    "    img_path = image_path\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    flatten_features = model_keras.predict(x)\n",
    "    return(flatten_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting features and creating dataframe \n",
    "\n",
    "train_data_df['img_features'] = [feature_extract_vgg19(x,model_keras = model) for x in train_data_df['image_paths']]\n",
    "\n",
    "val_data_df['img_features'] = [feature_extract_vgg19(x,model_keras = model) for x in val_data_df['image_paths']]\n",
    "\n",
    "print(train_data_df.shape)\n",
    "print(val_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Train and Validation features dataframes\n",
    "import pickle\n",
    "\n",
    "train_data_df.to_pickle(data_dir+'/train_data_df.pkl')\n",
    "val_data_df.to_pickle(data_dir+'/val_data_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Train,Validation feature dataframes and Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Saved Train and Validation feature dataframes\n",
    "import pickle\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "\n",
    "train_data_df = pd.read_pickle(data_dir+'/train_data_df.pkl')\n",
    "val_data_df = pd.read_pickle(data_dir+'/val_data_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting array column of features into multiple columns\n",
    "\n",
    "#feature_set = np.split(train_data_df['img_features'],len(train_data_df),axis = 0)\n",
    "#feature_set\n",
    "\n",
    "X_train = train_data_df.img_features.apply(pd.Series)\n",
    "y_train = train_data_df['class_name'].astype('category')\n",
    "\n",
    "X_val = val_data_df.img_features.apply(pd.Series)\n",
    "y_val = val_data_df['class_name'].astype('category')\n",
    "\n",
    "X_train_val = pd.concat([X_train,X_val])\n",
    "y_train_val = pd.concat([y_train,y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(df3, train_data_df['class_name'], random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_train_val.shape)\n",
    "print(y_train_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "  \n",
    "# training a linear SVM classifier \n",
    "#svm_model_linear = SVC(kernel = 'linear', C = 1, verbose = 1)\n",
    "\n",
    "svm_model_linear = SVC(kernel = 'rbf', C = 10, verbose = 1,probability = True)\n",
    "\n",
    "clf_fit = svm_model_linear.fit(X_train_val, y_train_val)\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#training randomforest\n",
    "#mdl_rf = RandomForestClassifier(n_estimators=1000,random_state=0,verbose=1,n_jobs=-1, min_samples_split= 2, min_samples_leaf= 1, max_features= 'auto', max_depth= 60, bootstrap= False)\n",
    "    \n",
    "#clf_fit = mdl_rf.fit(X_train_val, y_train_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GridSearch for SVM and RandomForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.svm import SVC \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#param_grid = {'C': [0.1,1, 10, 100],'kernel': ['rbf', 'linear', 'sigmoid']}\n",
    "\n",
    "#grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
    "#grid.fit(X_train_val,y_train_val)\n",
    "#print(grid.best_estimator_)\n",
    "\n",
    "\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [5,30,90],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [2,5,10],\n",
    "    'min_samples_split': [5,10,15,100],\n",
    "    'n_estimators': [500,1000,1500],\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier(random_state=0,verbose=1,n_jobs=-1)\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using RamdomizedSearchCV for RF parameter tuning\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "              'n_jobs': [-1]}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = None)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train_val, y_train_val)\n",
    "\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix \n",
    "# model accuracy for X_test   \n",
    "#predictions = clf_fit.predict(X_train) \n",
    "  \n",
    "# creating a confusion matrix \n",
    "#cm = confusion_matrix(y_train, predictions) \n",
    "#print(cm)\n",
    "\n",
    "#from sklearn.metrics import classification_report \n",
    "\n",
    "#print(classification_report(y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# calibrate model on validation data\n",
    "#calibrator = CalibratedClassifierCV(clf_fit, cv='prefit').fit(X_val, y_val)\n",
    "\n",
    "# evaluate the model\n",
    "#yhat = calibrator.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving SVM and Calibrator\n",
    "#clf_fit.to_pickle(data_dir+'/rf_trained.pkl')\n",
    "#calibrator.to_pickle(data_dir+'/calibrator_trained.pkl')\n",
    "\n",
    "#pickle.dump(clf_fit, open(data_dir+'/rf_trained.sav', 'wb'))\n",
    "\n",
    "pickle.dump(clf_fit, open(data_dir+'/svm_trained_prob.sav', 'wb'))\n",
    "\n",
    "#pickle.dump(calibrator, open(data_dir+'/calibrator_trained.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Test data,extracting Features and using saved model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list of all image paths for test data\n",
    "from os import walk\n",
    "from os.path import normpath, basename\n",
    "import pickle\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "test_path_list = []\n",
    "test_dir_list = []\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "test_data_dir = os.path.join(data_dir, 'test_data_20')\n",
    "\n",
    "#Generating Train data path list\n",
    "for (dirpath, dirnames, filenames) in walk(test_data_dir):\n",
    "    for filename in filenames:\n",
    "        test_dir_list.append(basename(normpath(dirpath)))\n",
    "        test_path_list.append(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting lists into dataframes\n",
    "test_data_df = pd.DataFrame()\n",
    "test_data_df['image_paths'] = test_path_list\n",
    "test_data_df['class_name'] = test_dir_list\n",
    "\n",
    "print(test_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for extracting features using VGG19 for a given image path\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "base_model = VGG19(weights='imagenet')\n",
    "#print(base_model.summary())\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)\n",
    "\n",
    "def feature_extract_vgg19(image_path,model_keras):\n",
    "\n",
    "    img_path = image_path\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    flatten_features = model_keras.predict(x)\n",
    "    return(flatten_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting features and creating dataframe\n",
    "\n",
    "test_data_df['img_features'] = [feature_extract_vgg19(x,model_keras = model) for x in test_data_df['image_paths']]\n",
    "\n",
    "test_data_df.to_pickle(data_dir+'/test_data_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Saved Test feature dataframe\n",
    "import pickle\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "\n",
    "test_data_df = pd.read_pickle(data_dir+'/test_data_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Saved Models\n",
    "#svm_model_linear = pickle.load(data_dir+'/svm_trained.pkl')\n",
    "#calibrator = pickle.load(data_dir+'/calibrator_trained.pkl')\n",
    "\n",
    "clf_fit = pickle.load(open(data_dir+'/rf_trained.sav', 'rb')) \n",
    "\n",
    "#clf_fit = pickle.load(open(data_dir+'/svm_trained.sav', 'rb'))\n",
    "#clf_fit = pickle.load(open(data_dir+'/svm_trained_prob.sav', 'rb'))\n",
    "#calibrator = pickle.load(open(data_dir+'/calibrator_trained.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data_df.img_features.apply(pd.Series)\n",
    "y_test = test_data_df['class_name'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=12)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on test data\n",
    "yhat_clf = clf_fit.predict(X_test)\n",
    "#yhat_calibrator = calibrator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a confusion matrix on predictions\n",
    "#from sklearn.metrics import confusion_matrix \n",
    "#cm = confusion_matrix(y_test, yhat)\n",
    "#print(cm)\n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "print(classification_report(y_test, yhat_clf))\n",
    "print(accuracy_score(y_test, yhat_clf))\n",
    "\n",
    "#print(classification_report(y_test, yhat_calibrator))\n",
    "#print(accuracy_score(y_test, yhat_calibrator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_fit.classes_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=12)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "yhat_clf_prob = clf_fit.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7530, 3)\n",
      "    top1     top2    top3\n",
      "0  video  weather  coffee\n",
      "1  video  weather  coffee\n",
      "2  video  weather  coffee\n",
      "3  video  weather  coffee\n",
      "4  video  weather  coffee\n",
      "5  video  weather  coffee\n",
      "6  video  weather  coffee\n",
      "7  video  weather  coffee\n",
      "8  video  weather  coffee\n",
      "9  video  weather  coffee\n",
      "(7530, 4)\n",
      "  class_name   top1     top2    top3\n",
      "0     coffee  video  weather  coffee\n",
      "1     coffee  video  weather  coffee\n",
      "2     coffee  video  weather  coffee\n",
      "3     coffee  video  weather  coffee\n",
      "4     coffee  video  weather  coffee\n",
      "5     coffee  video  weather  coffee\n",
      "6     coffee  video  weather  coffee\n",
      "7     coffee  video  weather  coffee\n",
      "8     coffee  video  weather  coffee\n",
      "9     coffee  video  weather  coffee\n"
     ]
    }
   ],
   "source": [
    "top_n_predictions = np.argsort(yhat_clf_prob, axis = 1)[:,-3:]\n",
    "    \n",
    "#then find the associated SOC code for each prediction\n",
    "top_class = clf_fit.classes_[top_n_predictions]\n",
    "top_class_df = pd.DataFrame(data=top_class,columns=['top1','top2','top3'])\n",
    "print(top_class_df.shape)\n",
    "print(top_class_df.head(10))\n",
    "\n",
    "    #merge it up with the validation labels and descriptions\n",
    "results = pd.merge(pd.DataFrame(y_test), top_class_df, left_index=True, right_index=True)\n",
    "print(results.shape)\n",
    "print(results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
