{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to remove EXIF data from images.\n",
    "import glob\n",
    "import os\n",
    "\n",
    "dirs = glob.glob(os.path.join('./BlobStorage/test_data/*/*jpg'))\n",
    "\n",
    "import matplotlib.image as mpimg \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "for file in dirs:\n",
    "    img = mpimg.imread(file) \n",
    "\n",
    "\n",
    "import piexif\n",
    "piexif.remove(img)\n",
    "\n",
    "#Refer from here\n",
    "#https://stackoverflow.com/questions/51219159/remove-exif-from-all-files-in-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a subset of only 20 classes for test.\n",
    "#import pandas as pd\n",
    "import pickle\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "f = open(\"fasttext/clusterCenters.txt\",'r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "\n",
    "line = lines[0].split()\n",
    "print(line)\n",
    "modelName = line[0]\n",
    "classesNow = line[1:]\n",
    "print(classesNow)\n",
    "f.close()\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "test_data_dir = os.path.join(data_dir, 'test_data') # the categories need to be in folders\n",
    "test_data_20_dir = os.path.join(data_dir, 'test_data_20') # the categories need to be in folders\n",
    "\n",
    "for class_name in classesNow:\n",
    "    shutil.copytree(test_data_dir+'/'+class_name,test_data_20_dir+'/'+class_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/karan1234/notebooks/Karan-Paper/BlobStorage/test_data_20\n",
      "Found 7530 images belonging to 20 classes.\n",
      "nb_samples 7530\n",
      "bag/000180340_original_1024x1024.jpg.jpg\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'BlobStorage')\n",
    "test_data_dir = os.path.join(data_dir, 'test_data_20') # the categories need to be in folders\n",
    "print(test_data_dir)\n",
    "\n",
    "img_width,img_height = 224, 224\n",
    "batch_size = 16\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255) \n",
    "\n",
    "test_generator = test_datagen.flow_from_directory( \n",
    "        test_data_dir, \n",
    "        target_size=(img_width, img_height), \n",
    "        color_mode = 'rgb',\n",
    "        batch_size=batch_size, \n",
    "        class_mode='categorical',\n",
    "        #class_mode=None,\n",
    "        shuffle = False) \n",
    "\n",
    "test_generator.reset()\n",
    "\n",
    "#Getting list of stored models in trained_models folder\n",
    "#models_list = [l for l in os.listdir(\"trained_models\") if l.endswith('.h5')]\n",
    "#print(models_list)\n",
    "\n",
    "#model = load_model(\"model_contextobject_4classes.h5\")\n",
    "\n",
    "filenames =  test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "print('nb_samples '+str(nb_samples))\n",
    "print(filenames[1])\n",
    "#predictions = model.predict_generator(test_generator, steps=nb_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0404 22:01:25.096070 140410433156864 deprecation_wrapper.py:119] From /anaconda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0404 22:01:25.117036 140410433156864 deprecation_wrapper.py:119] From /anaconda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0404 22:01:25.142859 140410433156864 deprecation_wrapper.py:119] From /anaconda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model20', 'sunset', 'railway', 'river', 'shoulder', 'bike', 'painting', 'coin', 'coffee', 'championship', 'bag', 'football', 'photograph', 'scene', 'weather', 'shore', 'room', 'car', 'dress', 'roof', 'video']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0404 22:01:25.449330 140410433156864 deprecation_wrapper.py:119] From /anaconda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0404 22:01:25.450126 140410433156864 deprecation_wrapper.py:119] From /anaconda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0404 22:01:25.450671 140410433156864 deprecation_wrapper.py:119] From /anaconda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0404 22:01:26.618477 140410433156864 deprecation_wrapper.py:119] From /anaconda/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0404 22:01:26.719737 140410433156864 deprecation.py:323] From /anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471/470 [==============================] - 84s 177ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "f = open(\"fasttext/clusterCenters.txt\",'r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.split()\n",
    "    print(line)\n",
    "    modelName = line[0]\n",
    "    classesNow = line[1:]\n",
    "    \n",
    "    #Keras sorts the list of classes used for training\n",
    "    classesNow.sort()\n",
    "    \n",
    "    model = load_model('trained_models/'+modelName+'.h5')\n",
    "    predictions = model.predict_generator(test_generator, steps=nb_samples/batch_size, verbose = 1)\n",
    "    pred_df = pd.DataFrame(data=predictions, index=filenames, columns=classesNow)\n",
    "    #print(pred_df.head())\n",
    "    #print(pred_df.shape)\n",
    "    pred_df.to_csv('predictions/all_categories/'+modelName+'.txt', header=True, index=True, sep=',',mode = 'w+')\n",
    "    \n",
    "    #Finding highest probability category\n",
    "    pred_df['max_prob'] = pred_df.idxmax(axis=1)\n",
    "    pred_df['max_prob'].to_csv('predictions/'+modelName+'.txt', header=True, index=True, sep=',',mode = 'w+')\n",
    "    #pred_df.to_pickle('predictions/'+modelName+'.pkl') \n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379\n",
      "[-0.092271, -0.14855, -0.14696, 0.013, -0.40305, -0.31004, 0.1022, -0.42087, -0.22948, 0.12853, -0.34886, 0.072545, 0.081316, -0.13273, -0.011657, 0.18024, -0.14901, -0.090313, -0.12595, 0.35055, -0.094697, -0.094339, -0.47509, -0.37384, 0.063611, 0.056118, -0.32637, 0.22324, -0.2292, 0.24179, 0.017067, 0.24906, -0.20583, 0.14183, -0.20341, 0.18192, -0.22186, -0.10049, 0.02935, 0.039431, -0.39592, -0.27336, -0.027534, -0.14903, 0.044646, 0.2911, -0.06559, -0.21112, 0.51928, -0.1435, 0.096369, -0.16417, 0.11613, 0.16193, -0.15615, 0.038471, 0.33975, 0.60247, 0.18831, 0.2651, 0.21832, -0.0048556, 0.29734, 0.057363, -0.23942, -0.21652, -0.3096, 0.19925, 0.23067, -0.13709, -0.28048, 0.31534, 0.13813, -0.24841, 0.02902, 0.12314, -0.13154, 0.33097, -0.035216, 0.35946, 0.11113, 0.1922, -0.37032, -0.21848, -0.28887, 0.074491, -0.14468, -0.21976, 0.29048, 0.0011665, 0.0023941, -0.17978, -0.13347, 0.17614, 0.25683, -0.2448, -0.2018, 0.015822, -0.18401, -0.33955, -0.25707, -0.29338, -0.0050284, -0.19509, 0.30135, 0.0071247, -0.067363, -0.11976, -0.063657, 0.39523, 0.12608, 0.21758, -0.41854, -0.12186, 0.3597, -0.034373, 0.38312, 0.12563, -0.41637, -0.14925, 0.421, 0.0878, 0.44853, 0.070908, 0.15227, 0.10244, 0.27376, -0.03661, 0.56322, 0.31124, 0.23129, -0.00025623, 0.097751, 0.18182, -0.14222, 0.25334, -0.076419, 0.1471, -0.31933, 0.21427, -0.044735, 0.18025, 0.13185, 0.095557, 0.32003, 0.029514, -0.0757, -0.11872, -0.0050592, 0.079976, 0.24998, 0.10069, -0.092863, -0.13386, 0.003857, 0.22484, -0.11086, -0.7529, -0.15102, -0.25202, 0.12113, -0.058052, -0.42751, 0.14419, 0.14954, -0.066018, -0.30967, 0.26718, 0.35141, -0.058832, 0.22867, -0.21641, -0.31999, -0.17233, -0.39197, 0.27359, 0.13471, 0.33385, 0.14807, -0.28704, -0.10228, -0.30756, -0.11438, -0.12307, -0.051538, 0.072152, 0.078797, 0.026715, 0.28872, -0.18068, -0.13672, -0.021216, 0.41264, 0.39031, -0.0831, -0.19714, 0.15815, 0.085991, 0.0028989, -0.21237, 0.053219, 0.25189, 0.23713, -0.18722, -0.30714, -0.32575, -0.25098, -0.37788, 0.45559, -0.19605, 0.48777, -0.017715, -0.31457, 0.056857, -0.28057, 0.25909, -0.54517, -0.47031, 0.088499, 0.09751, -0.13351, 0.030737, -0.048172, 0.30919, -0.1283, 0.10677, 0.16757, 0.16062, -0.043784, 0.05037, -0.23407, -0.46864, 0.11478, -0.30942, -0.32957, -0.043381, -0.042646, 0.33729, -0.11565, 0.0039308, 0.22447, -0.010812, 0.41574, 0.35257, -0.19972, 0.033721, -0.14266, -0.49707, 0.10887, -0.27352, -0.10215, -0.21594, -0.44397, -0.25398, 0.23433, -0.35625, 0.092468, -0.53119, -0.095438, 0.27416, 0.035904, -0.18139, -0.056535, -0.4238, -0.34544, -0.0059668, -0.32102, -0.21379, 0.25475, -0.089061, -0.50595, -0.037508, -0.0068223, 0.15243, -0.58001, -0.53294, 0.061134, 0.43995, 0.1875, 0.062242, 0.10616, 0.016311, 0.13847, -0.064991, 0.43318, 0.37801, 0.074758, -0.038963, -0.13571, -0.050288, 0.096352, 0.031328, 0.31818, -0.18818, 0.14998, -0.18162, -0.35564, 0.28245, -0.18557, -0.060884]\n"
     ]
    }
   ],
   "source": [
    "#Finding closest words to top predictions on testing set\n",
    "import math\n",
    "import pickle\n",
    "from scipy.spatial import distance\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "def scipy_distance(v, u):\n",
    "    return distance.euclidean(v, u)\n",
    "\n",
    "#Reading the fasttext dictionary populated at clustering phase\n",
    "fastext_dict = pickle.load(open(\"fasttext/fastext_dict.pkl\",\"rb\"))\n",
    "print(len(fastext_dict))\n",
    "#print(fastext_dict.keys())\n",
    "print(fastext_dict['car'])\n",
    "\n",
    "total_classes = 379\n",
    "\n",
    "dict_keys = list(fastext_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(fastext_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model20', 'sunset', 'railway', 'river', 'shoulder', 'bike', 'painting', 'coin', 'coffee', 'championship', 'bag', 'football', 'photograph', 'scene', 'weather', 'shore', 'room', 'car', 'dress', 'roof', 'video']\n",
      "20\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "#Generating the close words dictionary for each model\n",
    "f = open(\"fasttext/clusterCenters.txt\",'r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    line = line.split()\n",
    "    print(line)\n",
    "    modelName = line[0]\n",
    "    model_k = int(modelName[5:])\n",
    "    print(model_k)\n",
    "    classesNow = line[1:]\n",
    "    \n",
    "    closeWords_Count = math.ceil(total_classes/model_k)\n",
    "    print(closeWords_Count)\n",
    "    \n",
    "    closeWord_dict = {}\n",
    "    \n",
    "    for word in classesNow:\n",
    "        distance_dict = {}\n",
    "        \n",
    "        for fast_word in dict_keys:\n",
    "            dist = scipy_distance(fastext_dict[word],fastext_dict[fast_word])\n",
    "            distance_dict[fast_word] = dist\n",
    "            \n",
    "        #sorted_distace_dict = {k: v for k, v in sorted(distance_dict.items(), key=lambda item: item[1],reverse = True)[:closeWords_Count+1]}\n",
    "        closeWords_dict = {k: v for k, v in sorted(distance_dict.items(), key=lambda item: item[1])[:closeWords_Count+1]}\n",
    "        \n",
    "        #print(len(closeWords_dict))\n",
    "        #closeWords_dict = take(closeWords_Count+1, sorted_distace_dict.items())\n",
    "        closeWords_dict_keys = list(closeWords_dict.keys())\n",
    "        \n",
    "        closeWord_dict[word] = closeWords_dict_keys\n",
    "    \n",
    "    p = open(modelName+\"_closeWord_dict.pkl\",\"wb\")\n",
    "    pickle.dump(closeWord_dict,p)\n",
    "    p.close()\n",
    "           \n",
    "    #pred_df = pd.read_csv('predictions/'+modelName+'.txt', header=True, index=True, sep=',')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#closeWord_dict_list = [l for l in os.listdir(\"trained_models\") if l.endswith('.pkl')]\n",
    "#print(models_list)\n",
    "\n",
    "\n",
    "\n",
    "closeWord_dict = pickle.load(open(\"model50_closeWord_dict.pkl\",\"rb\"))\n",
    "print(len(closeWord_dict))\n",
    "print(closeWord_dict.keys())\n",
    "\n",
    "#print(closeWord_dict.keys())\n",
    "closeWord_dict['water']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model20\n",
      "471\n",
      "7530\n",
      "accuracy\n",
      "0.06254980079681274\n"
     ]
    }
   ],
   "source": [
    "#Running final predictions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def prediction_check(y_pred,y_act):\n",
    "    if y_act in y_pred:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "f = open(\"fasttext/clusterCenters.txt\",'r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    line = line.split()\n",
    "    #print(line)\n",
    "    modelName = line[0]\n",
    "    print(modelName)\n",
    "    #model_k = int(modelName[5:])\n",
    "    #print(model_k)\n",
    "    classesNow = line[1:]\n",
    "    \n",
    "    #Reading the predictions for each model\n",
    "    pred_df = pd.read_csv('predictions/'+modelName+'.txt', header=0, sep=',')\n",
    "    #print(pred_df.head)\n",
    "    pred_df.columns = ['test_file','max_prob']\n",
    "    pred_df['actual_label'] = pred_df.test_file.apply(lambda x: x.split('/')[0])\n",
    "    #print(pred_df.columns)\n",
    "    #print(pred_df.head())\n",
    "    \n",
    "    closeWord_dict = pickle.load(open(modelName+\"_closeWord_dict.pkl\",\"rb\"))\n",
    "    \n",
    "    \n",
    "    #closeWord_df = pd.DataFrame.from_dict(closeWord_dict, orient='index')\n",
    "    #closeWord_df.columns = ['test_file','max_prob']\n",
    "    #print(closeWord_df.columns)\n",
    "    \n",
    "    pred_df['guesses'] = pred_df['max_prob'].map(closeWord_dict)\n",
    "    \n",
    "    #pred_df['pred_check'] = pred_df.apply(lambda x: prediction_check(pred_df['guesses'],pred_df['actual_label']),axis = 1)\n",
    "    \n",
    "    pred_check = []\n",
    "    \n",
    "    #pred_df['pred_check'] = np.where(pred_df['actual_label'] in pred_df['guesses'],1,0)\n",
    "    for index,row in pred_df.iterrows():\n",
    "        if row['actual_label'] in row['guesses']:\n",
    "            #print('actual label '+ row['actual_label'])\n",
    "            #print('guesses '+ ' '.join(row['guesses']))\n",
    "            pred_check.append(1)\n",
    "        else:\n",
    "            pred_check.append(0)\n",
    "        \n",
    "    pred_df['pred_check'] = pred_check\n",
    "    \n",
    "    total_right = pred_df['pred_check'].sum()\n",
    "    print(total_right)\n",
    "    \n",
    "    total_rows = len(pred_df)\n",
    "    print(total_rows)\n",
    "    \n",
    "    print('accuracy')\n",
    "    print(total_right/total_rows)\n",
    "    \n",
    "    #print(pred_df.head())\n",
    "    #df['combined'] = df[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    \n",
    "    #print(closeWord_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
