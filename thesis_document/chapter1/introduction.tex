\chapter{}{Introduction}{Introduction}

Advances in deep neural networks have empowered machines to achieve human level classification performance on object recognition tasks. Very powerful and robust visual classifier frameworks have been developed, and will no doubt keep improving. In typical object recognition tasks, it is necessary to establish a certain number of predetermined object categories so that classification accuracy can be improved by collecting as many training image samples as possible for each object category. Many problem domains are faced with a large and growing number of object categories. As a consequence, it is becoming increasingly difficult to collect and annotate training data for each object category. Moreover, these images need to capture different aspects of the objects under various imaging conditions to account for the natural variance in appearance for each object category. The problem thus lies in collecting and annotating training data in an efficient and reliable manner for a wide variety of object categories. In addition, trained classifiers can only classify observed object instances into the classes or categories covered by the training data; they lack the ability to deal with previously unseen classes. To address this issue, zero-shot learning (ZSL) techniques have been proposed in the research literature. ZSL frameworks are designed to  tackle the problem of learning classifiers when no explicit visual training examples are provided.

\par
\medskip

Human beings perform ZSL naturally, enabling recognition of at least 30,000 object classes~\cite{humanimageunderstanding}. When faced with a new unfamiliar object, we are, after a while, able to state what it resembles: "A New York City hot dog cart, with the large block being the central food storage and cooking area, the rounded part underneath as a wheel, the large arc on the right as a handle, the funnel as an orange juice squeezer and the various vertical pipes as vents or umbrella supports." It is not a good cart, but we can see how it might be related to one~\cite{humanimageunderstanding}. For humans, it is as easy as recognizing a 10-letter word with 3 wrong letters. However, in the case of machines, we need a vast number of training images for each type of cart to learn to adapt to the naturally occurring variations in cart appearances. In humans, the ability to understand natural variations comes from our existing and ever evolving language knowledge base, which enables us to connect unseen categories with seen categories using high-level descriptions.

\par
\medskip

To emulate the ZSL process in machines, previously unseen object categories are recognized by leveraging auxiliary information related to categories. Auxiliary information are derived from external data sources such Wikipedia, WordNet~\cite{wordnet} etc. which make it analogous to the human (natural language) knowledge base. As the auxiliary inputs usually carry semantic information, they constitute a \textit{semantic space}. A typical source of semantic information used in ZSL is \textit{attribute spaces}. Attribute spaces are semantic spaces that are engineered manually for each domain or data set. Attributes are a list of terms that describe various properties of each object class or category. For example, an attribute could be \textit{hair color} with values "black", "brown", "white" etc. The attributes can be either discrete or continuous. Label-embedding spaces are also often used as a source of semantic information, where the word/label representations are obtained by employing information retrieval techniques on large digital text corpora. Examples of widely used label-embedding models include Word2Vec~\cite{w2v}, GloVe~\cite{glove}, and FastText~\cite{fasttext}. Hierarchical information is another source of semantic information that can be derived from a pre-existing ontology such as WordNet \cite{wordnet}. All sources of auxiliary information when combined together comprise the semantic space.

\par
\medskip

In typical ZSL frameworks, a set of previously observed classes is used to train the visual classifier. These classes are termed as \textit{seen classes}. The framework is then evaluated on another set of previously not observed classes termed as \textit{unseen classes}. While training, the classifier has access to auxiliary information of both the seen and unseen classes. A formal definition of the ZSL task is provided in Definition~\ref{def:zsl}. 
Conventional ZSL is restrictive in its formulation since it assumes that the input images at the time of  prediction or inference can only come from the unseen classes. In contrast, generalized ZSL addresses the more general setting where the input images at the time of  prediction or inference can come from both, the seen and unseen classes~\cite{gen-zsl}. Generalized ZSL is formally defined in Definition \ref{def:gzsl}. 

\par
\medskip

\theoremstyle{definition}
\begin{definition}[Conventional Zero-shot Learning]
\label{def:zsl}
Given labelled training instances $X_s$ belonging to seen classes $Y_s$, zero-shot learning aims to learn a classifier that can classify testing instances $X_u$ belonging to the unseen classes $Y_u$
\end{definition}

\par
\medskip

\theoremstyle{definition}
\begin{definition}[Generalised Zero-shot Learning]
\label{def:gzsl}
Given labelled training instances $X_s$ belonging to seen classes $Y_s$, generalised zero-shot learning aims to learn a classifier that can classify testing instances $X_{u \cup s}$ belonging to the classes $Y_u \cup  Y_s$
\end{definition}

\par
\medskip

Several ZSL frameworks have been proposed in the literature, however, all of these frameworks use a proposed split~\cite{gbu} of standard ZSL data sets~\cite{awa,cub,sun} into seen and unseen classes. This split is formulated in to aid uniform research towards finding a universal ZSL framework that outperforms the existing ones. Altogether, the zero-shot learning problem has been formally framed for each standard data set using specific categories as seen classes and the remaining as unseen classes in a race for attaining maximum classification accuracy. Of the total object categories present in each data set, the number of seen classes has always been significantly higher than the number of unseen classes in most ZSL frameworks. For example, the \textit{Animals-with-Attributes} (AWA2) data set~\cite{awa} has a proposed 40:10 seen:unseen class split, the \textit{Caltech-USCD-Birds} (CUB) data set~\cite{cub} has a 150:50 seen:unseen class split, and the large-scale \textit{Scene Understanding} (SUN) database~\cite{sun} has a 645:72 seen:unseen class split. While this formulation has helped to formulate several benchmark approaches to ZSL tasks, we notice that the original intent of mitigating the data collection process has been skirted at a very early stage. Therefore, we aim to infer larger number of unseen object categories using very few seen object categories. We believe this addresses the original problem of obtaining annotated images to a greater extent. 

\par
\medskip

We propose a new framework that helps us to examine the limits of inferring unseen object categories from very few seen object categories, i.e., test the limits of ZSL. We note the functional dependence of the classification accuracy on the number of previously seen classes across the spectrum of the classes on three widely used object classification data sets~\cite{awa,cub,sun}. An important contribution of the proposed approach is its ability to determine the optimal set of representative classes using which one could infer a large number of previously unseen classes with a pre-specified measure of accuracy. We explore intuitive techniques to select a few seen classes which would enable us to predict a larger number of unseen classes. The proposed approach also aids the training data collection process significantly by identifying the key object categories from which the training data collection process can be initiated and determining which object categories to stop at, based on an expected or pre-specified classification accuracy measure for a specific problem. We evaluate the proposed approach in the generalized ZSL setting, thus making it very practical. We present valuable insights into the inference process for general and specific cases where the proposed approach performs exceptionally well, and also for cases where we fail to infer the correct unseen category. We also compare the proposed approach with the well known Attribute Label Embedding (ALE)~\cite{ale} procedure, which has been shown to perform very well on the aforementioned three standard data sets as published in~\cite{gbu}. In comparison to ALE, we observe that the proposed approach achieves 21\% higher accuracy on the AWA2 data set, 6\% higher accuracy on the CUB data set and comparable performance on the SUN data set. We also establish the minimum number of previously seen classes needed to obtain reasonable (or above average) generalized ZSL performance on the AWA2 data set as 20 seen classes out of a total of 50 classes, on the CUB data set as 80 seen classes out of a total of 200 classes and on the SUN data set as 360 seen classes out of a total of 717 classes.

\par
\medskip

This thesis is organized into six chapters. Chapter 1 introduces the concept of zero-shot learning (ZSL) and summarizes the work done in this thesis. Chapter 2 reviews the related work in ZSL and position our work in the overall ZSL research literature. Chapter 3 describes the various data sets used for experiments carried out in this thesis. Chapter 4 discusses the overall methodology underlying the proposed approach and also explains the finer details about the methods used. Chapter 5 presents the experimental results of the proposed approach on the aforementioned three data sets. Chapter 5 compares the proposed approach with the Attribute Label Embedding (ALE) scheme and shows how well the proposed approach fares in comparison to the widely used ALE-based ZSL framework. Finally, in Chapter 6, we conclude this thesis and discuss directions for future work.